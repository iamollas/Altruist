{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# This notebook contains the experiments on Adult Census dataset with Altruist"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Load few libraries we will need"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import warnings\n",
    "warnings.filterwarnings(\"ignore\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Using TensorFlow backend.\n"
     ]
    }
   ],
   "source": [
    "from sklearn.base import TransformerMixin\n",
    "from sklearn.preprocessing import MinMaxScaler,StandardScaler, LabelEncoder\n",
    "from sklearn.pipeline import Pipeline\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.ensemble import GradientBoostingClassifier\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "from sklearn.neural_network import MLPClassifier\n",
    "from sklearn.model_selection import GridSearchCV\n",
    "from sklearn.svm import SVC\n",
    "from sklearn.model_selection import train_test_split\n",
    "from altruist import Altruist\n",
    "from fi_techniques import FeatureImportance\n",
    "import pandas as pd \n",
    "import numpy as np\n",
    "import urllib\n",
    "import networkx as nx\n",
    "import matplotlib.pyplot as plt\n",
    "from ipywidgets import interact, interactive, fixed, interact_manual\n",
    "import ipywidgets as widgets"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Data Loading"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Firstly, we load the dataset and we set the feature and class names"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "feature_names = ['age', 'workclass', 'fnlwgt', 'education', 'education-num', 'marital-status', 'occupation', 'relationship', 'race', 'sex', 'capital-gain', 'capital-loss', 'hours-per-week', 'native-country','salary']\n",
    "class_names=['<=50K','>50K'] #0: <=50K and 1: >50K\n",
    "data = pd.read_csv('https://archive.ics.uci.edu/ml/machine-learning-databases/adult/adult.data', names=feature_names, delimiter=', ')\n",
    "data_test = pd.read_csv('https://archive.ics.uci.edu/ml/machine-learning-databases/adult/adult.test', names=feature_names, delimiter=', ')\n",
    "data_test = data_test.drop(data_test.index[[0]])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We are doing the following preprocessing influenced by a github notebook"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "data = data[(data != '?').all(axis=1)]\n",
    "data_test = data_test[(data_test != '?').all(axis=1)]\n",
    "data_test['salary'] = data_test['salary'].map({'<=50K.': '<=50K', '>50K.': '>50K'})\n",
    "frames = [data, data_test]\n",
    "data = pd.concat(frames)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Feature Engineering from:\n",
    "https://github.com/pooja2512/Adult-Census-Income/blob/master/Adult%20Census%20Income.ipynb. So run and skip the next code block"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of observations in final dataset: (45167, 82)\n"
     ]
    }
   ],
   "source": [
    "hs_grad = ['HS-grad','11th','10th','9th','12th']\n",
    "elementary = ['1st-4th','5th-6th','7th-8th']\n",
    "# replace elements in list.\n",
    "for i in hs_grad:\n",
    "    data['education'].replace(i , 'HS-grad', regex=True , inplace=True)\n",
    "for e in elementary:\n",
    "    data['education'].replace(e , 'elementary-school', regex=True, inplace = True)\n",
    "\n",
    "married= ['Married-spouse-absent','Married-civ-spouse','Married-AF-spouse']\n",
    "separated = ['Separated','Divorced']\n",
    "#replace elements in list.\n",
    "for m in married:\n",
    "    data['marital-status'].replace(m ,'Married', regex=True, inplace = True)\n",
    "for s in separated:\n",
    "    data['marital-status'].replace(s ,'Separated', regex=True, inplace = True)\n",
    "\n",
    "self_employed = ['Self-emp-not-inc','Self-emp-inc']\n",
    "govt_employees = ['Local-gov','State-gov','Federal-gov']\n",
    "for se in self_employed:\n",
    "    data['workclass'].replace(se , 'Self_employed', regex=True, inplace = True)\n",
    "for ge in govt_employees:\n",
    "    data['workclass'].replace(ge , 'Govt_employees', regex=True, inplace = True)\n",
    "\n",
    "del_cols = ['relationship','education-num']\n",
    "data.drop(labels = del_cols, axis = 1, inplace = True)\n",
    "\n",
    "index_age = data[data['age'] == 90].index\n",
    "data.drop(labels = index_age, axis = 0, inplace =True)\n",
    "num_col_new = ['age','capital-gain', 'capital-loss',\n",
    "       'hours-per-week','fnlwgt']\n",
    "cat_col_new = ['workclass', 'education', 'marital-status', 'occupation',\n",
    "               'race', 'sex','salary','native-country']#add native-country label\n",
    "scaler = MinMaxScaler()\n",
    "#pd.DataFrame(scaler.fit_transform(data[num_col_new]),columns = num_col_new)\n",
    "class DataFrameSelector(TransformerMixin):\n",
    "    def __init__(self,attribute_names):\n",
    "        self.attribute_names = attribute_names\n",
    "    def fit(self,X,y = None):\n",
    "        return self\n",
    "    def transform(self,X):\n",
    "        return X[self.attribute_names]\n",
    "class num_trans(TransformerMixin):\n",
    "    def __init__(self):\n",
    "        pass\n",
    "    def fit(self,X,y=None):\n",
    "        return self\n",
    "    def transform(self,X):\n",
    "        df = pd.DataFrame(X)\n",
    "        df.columns = num_col_new \n",
    "        return df\n",
    "pipeline = Pipeline([('selector',DataFrameSelector(num_col_new)),  \n",
    "                     ('scaler',MinMaxScaler()),('transform',num_trans())])#('scaler',MinMaxScaler()),        \n",
    "num_df = pipeline.fit_transform(data)\n",
    "num_df.shape\n",
    "# columns which I don't need after creating dummy variables dataframe\n",
    "cols = ['workclass_Govt_employess','education_Some-college',\n",
    "        'marital-status_Never-married','occupation_Other-service',\n",
    "        'race_Black','sex_Male','salary_>50K']\n",
    "class dummies(TransformerMixin):\n",
    "    def __init__(self,cols):\n",
    "        self.cols = cols\n",
    "    \n",
    "    def fit(self,X,y = None):\n",
    "        return self\n",
    "    \n",
    "    def transform(self,X):\n",
    "        df = pd.get_dummies(X)\n",
    "        df_new = df[df.columns.difference(cols)] \n",
    "        return df_new\n",
    "pipeline_cat=Pipeline([('selector',DataFrameSelector(cat_col_new)),\n",
    "                      ('dummies',dummies(cols))])\n",
    "cat_df = pipeline_cat.fit_transform(data)\n",
    "cat_df['id'] = pd.Series(range(cat_df.shape[0]))\n",
    "num_df['id'] = pd.Series(range(num_df.shape[0]))\n",
    "final_df = pd.merge(cat_df,num_df,how = 'inner', on = 'id')\n",
    "print(f\"Number of observations in final dataset: {final_df.shape}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We extract the train and target data from the dataframe"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "y = final_df['salary_<=50K'].values\n",
    "final_df.drop(labels = ['id','salary_<=50K'],axis = 1,inplace = True)\n",
    "X = final_df.values"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "feature_names = list(final_df.columns.values)\n",
    "categorical_features = ['workclass', 'education', 'marital-status', 'occupation', 'race', 'sex','native-country']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "45167"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(X)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Original dataset shape Counter({1: 33970, 0: 11197})\n",
      "TomekLinks: Resampled dataset shape Counter({1: 31155, 0: 11197})\n",
      "NC: Resampled dataset shape Counter({1: 25878, 0: 11197})\n",
      "NM: Resampled dataset shape Counter({0: 11197, 1: 7017})\n",
      "Random: Resampled dataset shape Counter({0: 7017, 1: 7017})\n"
     ]
    }
   ],
   "source": [
    "from imblearn.under_sampling import TomekLinks, NeighbourhoodCleaningRule, NearMiss,RandomUnderSampler\n",
    "from collections import Counter\n",
    "\n",
    "print('Original dataset shape %s' % Counter(y))\n",
    "\n",
    "tl = TomekLinks()\n",
    "X_res, y_res = tl.fit_resample(X, y)\n",
    "print('TomekLinks: Resampled dataset shape %s' % Counter(y_res))\n",
    "ncr = NeighbourhoodCleaningRule()\n",
    "X_res, y_res = ncr.fit_resample(X_res, y_res)\n",
    "print('NC: Resampled dataset shape %s' % Counter(y_res))\n",
    "nm = NearMiss(version=3)\n",
    "X_res, y_res = nm.fit_resample(X_res, y_res)\n",
    "print('NM: Resampled dataset shape %s' % Counter(y_res))\n",
    "rus = RandomUnderSampler(random_state=42)\n",
    "X_res, y_res = rus.fit_resample(X_res, y_res)\n",
    "print('Random: Resampled dataset shape %s' % Counter(y_res))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Random: Resampled dataset shape Counter({0: 1983, 1: 1983})\n"
     ]
    }
   ],
   "source": [
    "rus = RandomUnderSampler(sampling_strategy='all', random_state=42)\n",
    "X_res2, y_res2 = rus.fit_resample(X_res[:9000], y_res[:9000])\n",
    "print('Random: Resampled dataset shape %s' % Counter(y_res2))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Random: Resampled dataset shape Counter({0: 500, 1: 500})\n"
     ]
    }
   ],
   "source": [
    "rus = RandomUnderSampler(sampling_strategy='all', random_state=42)\n",
    "X_res2, y_res2 = rus.fit_resample(X_res[:7517], y_res[:7517])\n",
    "print('Random: Resampled dataset shape %s' % Counter(y_res2))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We have 1372 instances. We are going to use the build-in GridSearch of LionForests to find and train the best classifier for this dataset"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Machine Learning models training step"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We will use a MinMax scaler to normalize the input\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "scaler = MinMaxScaler(feature_range=(-1,1))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "classifiers = {}\n",
    "scalers = {}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Fitting 10 folds for each of 1 candidates, totalling 10 fits\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[Parallel(n_jobs=-1)]: Using backend LokyBackend with 8 concurrent workers.\n",
      "[Parallel(n_jobs=-1)]: Done   6 out of  10 | elapsed:    1.3s remaining:    0.9s\n",
      "[Parallel(n_jobs=-1)]: Done  10 out of  10 | elapsed:    1.4s finished\n"
     ]
    }
   ],
   "source": [
    "pipe = Pipeline(steps=[('scaler', scaler), ('rf', RandomForestClassifier(random_state=77))])\n",
    "parameters =[{\n",
    "    'rf__max_depth': [7], #1, 5, 7, 10\n",
    "    'rf__max_features': ['sqrt'], #'sqrt', 'log2', 0.75, None\n",
    "    'rf__bootstrap': [False], #True, False\n",
    "    'rf__min_samples_leaf' : [2], #1, 2, 5, 10, 0.10\n",
    "    'rf__n_estimators': [10] #10, 100, 500, 1000\n",
    "}]\n",
    "clf = GridSearchCV(pipe, parameters, scoring='f1', cv=10, n_jobs=-1, verbose=1)\n",
    "clf.fit(X_res2, y_res2)\n",
    "scaler_rf = clf.best_estimator_.steps[0][1]\n",
    "rf = clf.best_estimator_.steps[1][1]\n",
    "classifiers[1] = [rf, str(\"Random Forests: \"+ str(clf.best_score_))]\n",
    "scalers[1] = scaler_rf"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[Parallel(n_jobs=-1)]: Using backend LokyBackend with 8 concurrent workers.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Fitting 10 folds for each of 1 candidates, totalling 10 fits\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[Parallel(n_jobs=-1)]: Done   6 out of  10 | elapsed:    0.3s remaining:    0.2s\n",
      "[Parallel(n_jobs=-1)]: Done  10 out of  10 | elapsed:    0.5s finished\n"
     ]
    }
   ],
   "source": [
    "pipe = Pipeline(steps=[('scaler', scaler), ('svm', SVC(probability=True,random_state=77))])\n",
    "parameters = [\n",
    "  {'svm__C': [-3, 1, 3, 10, 100, 1000], 'svm__kernel': ['linear']},\n",
    "  {'svm__C': [-3, 1, 3, 10, 100, 1000], 'svm__gamma': [0.1, 0.01, 0.001, 0.0001], 'svm__kernel': ['rbf']},\n",
    "]\n",
    "parameters = {'svm__C': [3], 'svm__gamma': [0.1], 'svm__kernel': ['rbf']} #best\n",
    "clf = GridSearchCV(pipe, parameters, scoring='f1', cv=10, n_jobs=-1, verbose=1)\n",
    "clf.fit(X_res2, y_res2)\n",
    "scaler_svm = clf.best_estimator_.steps[0][1]\n",
    "svm = clf.best_estimator_.steps[1][1]\n",
    "classifiers[2] = [svm, str(\"SVM: \"+ str(clf.best_score_))]\n",
    "scalers[2] = scaler_svm"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Fitting 10 folds for each of 1 candidates, totalling 10 fits\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[Parallel(n_jobs=-1)]: Using backend LokyBackend with 8 concurrent workers.\n",
      "[Parallel(n_jobs=-1)]: Done   6 out of  10 | elapsed:    0.3s remaining:    0.2s\n",
      "[Parallel(n_jobs=-1)]: Done  10 out of  10 | elapsed:    0.3s finished\n"
     ]
    }
   ],
   "source": [
    "pipe = Pipeline(steps=[('scaler', scaler), ('lr', LogisticRegression(random_state=77))])\n",
    "parameters = [\n",
    "  {'lr__C': [-3, 1, 3, 10, 100, 1000], 'lr__penalty': ['l1'], 'lr__solver': ['liblinear', 'saga']},\n",
    "  {'lr__C': [-3, 1, 3, 10, 100, 1000], 'lr__penalty': ['l2'], 'lr__solver': ['newton-cg', 'lbfgs', 'sag','saga']}\n",
    "]\n",
    "parameters = {'lr__C': [3], 'lr__penalty': ['l2'], 'lr__solver': ['sag']}#best\n",
    "clf = GridSearchCV(pipe, parameters, scoring='f1', cv=10, n_jobs=-1, verbose=1)\n",
    "clf.fit(X_res2, y_res2)\n",
    "scaler_lr = clf.best_estimator_.steps[0][1]\n",
    "lr = clf.best_estimator_.steps[1][1]\n",
    "classifiers[3] = [lr, str(\"Logistic Regression: \"+ str(clf.best_score_))]\n",
    "scalers[3] = scaler_lr"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Fitting 10 folds for each of 1 candidates, totalling 10 fits\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[Parallel(n_jobs=-1)]: Using backend LokyBackend with 8 concurrent workers.\n",
      "[Parallel(n_jobs=-1)]: Done   6 out of  10 | elapsed:    1.1s remaining:    0.7s\n",
      "[Parallel(n_jobs=-1)]: Done  10 out of  10 | elapsed:    1.7s finished\n"
     ]
    }
   ],
   "source": [
    "pipe = Pipeline(steps=[('scaler', scaler), ('nn', MLPClassifier(early_stopping=True, random_state=77))])\n",
    "parameters = {\n",
    "    'nn__hidden_layer_sizes': [(2,10),(5,10),(10,100),(20,200),(50,500),(100,1000)], \n",
    "    'nn__activation': ['logistic', 'tanh', 'relu'],\n",
    "    'nn__solver': ['sgd', 'adam'],\n",
    "    'nn__alpha': [0.000001,0.0001,0.001, 0.01, 0.1],\n",
    "    'nn__learning_rate': ['constant', 'invscaling', 'adaptive']}\n",
    "parameters = {\n",
    "    'nn__hidden_layer_sizes': [(50,500)], \n",
    "    'nn__activation': ['tanh'],\n",
    "    'nn__solver': ['adam'],\n",
    "    'nn__alpha': [0.000001],\n",
    "    'nn__learning_rate': ['constant']}\n",
    "clf = GridSearchCV(pipe, parameters, scoring='f1', cv=10, n_jobs=-1, verbose=1)\n",
    "clf.fit(X_res2, y_res2)\n",
    "\n",
    "scaler_nn = clf.best_estimator_.steps[0][1]\n",
    "nn = clf.best_estimator_.steps[1][1]\n",
    "classifiers[4] = [nn, str(\"Neural Network: \"+ str(clf.best_score_))]\n",
    "scalers[4] = scaler_nn"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{1: [RandomForestClassifier(bootstrap=False, max_depth=7, max_features='sqrt',\n",
       "                         min_samples_leaf=2, n_estimators=10, random_state=77),\n",
       "  'Random Forests: 0.9431755994216005'],\n",
       " 2: [SVC(C=3, gamma=0.1, probability=True, random_state=77),\n",
       "  'SVM: 0.9593078770419098'],\n",
       " 3: [LogisticRegression(C=3, random_state=77, solver='sag'),\n",
       "  'Logistic Regression: 0.9467018455060752'],\n",
       " 4: [MLPClassifier(activation='tanh', alpha=1e-06, early_stopping=True,\n",
       "                hidden_layer_sizes=(50, 500), random_state=77),\n",
       "  'Neural Network: 0.9442092900222173']}"
      ]
     },
     "execution_count": 21,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "classifiers"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "34fded6ea2834e249f6476067b25d50d",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "interactive(children=(Checkbox(value=False, description='eli_5'), Checkbox(value=True, description='lime'), Ch…"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "@interact(eli_5=False, lime=True, shap=True, perm_importance=True, intristic=True, cl=(1,4))\n",
    "def g(eli_5, lime, shap, perm_importance, intristic, cl=2):\n",
    "    print(classifiers[cl][1])\n",
    "    X_t = scalers[cl].transform(X_res2)\n",
    "    fi = FeatureImportance(X_t[900:1000], y_res2[900:1000], feature_names, class_names)\n",
    "    fi_names = {fi.fi_lime:'Lime',fi.fi_shap:'Shap',fi.fi_eli:'Eli5',fi.fi_perm_imp:'Permuation Importance',fi.fi_rf:'Pseudo-Intristic RFs', fi.fi_coef_lr:'Intristic LR'}\n",
    "    fis = []\n",
    "    if (eli_5 and not cl == 2 and not cl == 3):\n",
    "        fis.append(fi.fi_eli)\n",
    "    if lime:\n",
    "        fis.append(fi.fi_lime)\n",
    "    if shap:\n",
    "        fis.append(fi.fi_shap)\n",
    "    if perm_importance:\n",
    "        fis.append(fi.fi_perm_imp)\n",
    "    if intristic and cl == 1:\n",
    "        fis.append(fi.fi_rf)\n",
    "    if intristic and cl == 3:\n",
    "        fis.append(fi.fi_coef_lr)\n",
    "    fis_scores = []\n",
    "    for i in fis:\n",
    "        fis_scores.append([])\n",
    "    count = 0;\n",
    "    altruistino = Altruist(classifiers[cl][0], X_t, fis, feature_names)\n",
    "    for instance in X_t[900:1000]:\n",
    "        if (count + 1) % 10 == 0:\n",
    "            print(count+1,\"/\",len(X_t),\"..\",end=\", \")\n",
    "        count = count + 1\n",
    "        untruthful_features = altruistino.find_untruthful_features(instance)\n",
    "        for i in range(len(untruthful_features[0])):\n",
    "            fis_scores[i].append(len(untruthful_features[0][i]))\n",
    "    print(len(X_t),\"/\",len(X_t))\n",
    "    count = 0\n",
    "    for fis_score in fis_scores:\n",
    "        fi = fis[count]\n",
    "        count = count + 1\n",
    "        print(fi_names[fi],np.array(fis_score).mean())\n",
    "        print(fi_names[fi],np.array(fis_score).sum())\n",
    "        print(count)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.8"
  },
  "toc": {
   "base_numbering": 1,
   "nav_menu": {},
   "number_sections": true,
   "sideBar": true,
   "skip_h1_title": false,
   "title_cell": "Table of Contents",
   "title_sidebar": "Contents",
   "toc_cell": false,
   "toc_position": {},
   "toc_section_display": true,
   "toc_window_display": false
  },
  "varInspector": {
   "cols": {
    "lenName": 16,
    "lenType": 16,
    "lenVar": 40
   },
   "kernels_config": {
    "python": {
     "delete_cmd_postfix": "",
     "delete_cmd_prefix": "del ",
     "library": "var_list.py",
     "varRefreshCmd": "print(var_dic_list())"
    },
    "r": {
     "delete_cmd_postfix": ") ",
     "delete_cmd_prefix": "rm(",
     "library": "var_list.r",
     "varRefreshCmd": "cat(var_dic_list()) "
    }
   },
   "types_to_exclude": [
    "module",
    "function",
    "builtin_function_or_method",
    "instance",
    "_Feature"
   ],
   "window_display": false
  },
  "widgets": {
   "application/vnd.jupyter.widget-state+json": {
    "state": {},
    "version_major": 2,
    "version_minor": 0
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
