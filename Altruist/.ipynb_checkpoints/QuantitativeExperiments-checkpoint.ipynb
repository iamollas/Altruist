{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Altruist Quantitative Experiments"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In this notebook we will provide a set of quantitative experiments using Altruist, on three different datasets, 4 different machine learning models and 4 different interpretation techniques."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Load few libraries we will need"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-09-21T06:29:11.312677Z",
     "start_time": "2020-09-21T06:29:11.308248Z"
    }
   },
   "outputs": [],
   "source": [
    "import warnings\n",
    "warnings.filterwarnings(\"ignore\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-09-21T06:29:14.758479Z",
     "start_time": "2020-09-21T06:29:12.276875Z"
    }
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Using TensorFlow backend.\n"
     ]
    }
   ],
   "source": [
    "from sklearn.base import TransformerMixin\n",
    "from sklearn.preprocessing import MinMaxScaler,StandardScaler, LabelEncoder\n",
    "from sklearn.pipeline import Pipeline\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.ensemble import GradientBoostingClassifier\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "from sklearn.neural_network import MLPClassifier\n",
    "from sklearn.model_selection import GridSearchCV\n",
    "from sklearn.svm import SVC\n",
    "from sklearn.model_selection import train_test_split\n",
    "from altruist import Altruist\n",
    "from fi_techniques import FeatureImportance\n",
    "import pandas as pd \n",
    "import numpy as np\n",
    "import urllib\n",
    "import networkx as nx\n",
    "import matplotlib.pyplot as plt\n",
    "from ipywidgets import interact, interactive, fixed, interact_manual\n",
    "import ipywidgets as widgets"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Banknote Dataset"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "About Banknote: It is a binary classification problem detecting real or fake banknotes. Data were extracted from images that were taken from genuine and forged banknote-like specimens. For digitization, an industrial camera usually used for print inspection was used. The final images have 400x 400 pixels. Due to the object lens and distance to the investigated object gray-scale pictures with a resolution of about 660 dpi were gained. Wavelet Transform tool were used to extract features from images. \n",
    "\n",
    "Attribute Information:\n",
    "\n",
    "1. variance of Wavelet Transformed image (continuous)\n",
    "2. skewness of Wavelet Transformed image (continuous)\n",
    "3. curtosis of Wavelet Transformed image (continuous)\n",
    "4. entropy of image (continuous) \n",
    "\n",
    "Source: https://archive.ics.uci.edu/ml/datasets/banknote+authentication"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Data Loading"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Firstly, we load the dataset and we set the feature and class names"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-09-21T06:29:17.151180Z",
     "start_time": "2020-09-21T06:29:16.828829Z"
    }
   },
   "outputs": [],
   "source": [
    "banknote_datadset = pd.read_csv('https://raw.githubusercontent.com/Kuntal-G/Machine-Learning/master/R-machine-learning/data/banknote-authentication.csv')\n",
    "feature_names = ['variance','skew','curtosis','entropy']\n",
    "class_names=['fake banknote','real banknote'] #0: no, 1: yes #or ['not authenticated banknote','authenticated banknote']"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We can plot some instances to see the features and their values"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-09-21T06:29:17.766368Z",
     "start_time": "2020-09-21T06:29:17.752109Z"
    }
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>variance</th>\n",
       "      <th>skew</th>\n",
       "      <th>curtosis</th>\n",
       "      <th>entropy</th>\n",
       "      <th>class</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>3.62160</td>\n",
       "      <td>8.6661</td>\n",
       "      <td>-2.8073</td>\n",
       "      <td>-0.44699</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>4.54590</td>\n",
       "      <td>8.1674</td>\n",
       "      <td>-2.4586</td>\n",
       "      <td>-1.46210</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>3.86600</td>\n",
       "      <td>-2.6383</td>\n",
       "      <td>1.9242</td>\n",
       "      <td>0.10645</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>3.45660</td>\n",
       "      <td>9.5228</td>\n",
       "      <td>-4.0112</td>\n",
       "      <td>-3.59440</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>0.32924</td>\n",
       "      <td>-4.4552</td>\n",
       "      <td>4.5718</td>\n",
       "      <td>-0.98880</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   variance    skew  curtosis  entropy  class\n",
       "0   3.62160  8.6661   -2.8073 -0.44699      0\n",
       "1   4.54590  8.1674   -2.4586 -1.46210      0\n",
       "2   3.86600 -2.6383    1.9242  0.10645      0\n",
       "3   3.45660  9.5228   -4.0112 -3.59440      0\n",
       "4   0.32924 -4.4552    4.5718 -0.98880      0"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "banknote_datadset.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Moreover, we can use pandas.describe() to see the ranges of each feature. For example, we observe that curtosis's range is -5.286 to 17.927"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-09-21T06:29:18.653220Z",
     "start_time": "2020-09-21T06:29:18.625726Z"
    }
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>variance</th>\n",
       "      <th>skew</th>\n",
       "      <th>curtosis</th>\n",
       "      <th>entropy</th>\n",
       "      <th>class</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>count</th>\n",
       "      <td>1372.000000</td>\n",
       "      <td>1372.000000</td>\n",
       "      <td>1372.000000</td>\n",
       "      <td>1372.000000</td>\n",
       "      <td>1372.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>mean</th>\n",
       "      <td>0.433735</td>\n",
       "      <td>1.922353</td>\n",
       "      <td>1.397627</td>\n",
       "      <td>-1.191657</td>\n",
       "      <td>0.444606</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>std</th>\n",
       "      <td>2.842763</td>\n",
       "      <td>5.869047</td>\n",
       "      <td>4.310030</td>\n",
       "      <td>2.101013</td>\n",
       "      <td>0.497103</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>min</th>\n",
       "      <td>-7.042100</td>\n",
       "      <td>-13.773100</td>\n",
       "      <td>-5.286100</td>\n",
       "      <td>-8.548200</td>\n",
       "      <td>0.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>25%</th>\n",
       "      <td>-1.773000</td>\n",
       "      <td>-1.708200</td>\n",
       "      <td>-1.574975</td>\n",
       "      <td>-2.413450</td>\n",
       "      <td>0.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>50%</th>\n",
       "      <td>0.496180</td>\n",
       "      <td>2.319650</td>\n",
       "      <td>0.616630</td>\n",
       "      <td>-0.586650</td>\n",
       "      <td>0.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>75%</th>\n",
       "      <td>2.821475</td>\n",
       "      <td>6.814625</td>\n",
       "      <td>3.179250</td>\n",
       "      <td>0.394810</td>\n",
       "      <td>1.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>max</th>\n",
       "      <td>6.824800</td>\n",
       "      <td>12.951600</td>\n",
       "      <td>17.927400</td>\n",
       "      <td>2.449500</td>\n",
       "      <td>1.000000</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "          variance         skew     curtosis      entropy        class\n",
       "count  1372.000000  1372.000000  1372.000000  1372.000000  1372.000000\n",
       "mean      0.433735     1.922353     1.397627    -1.191657     0.444606\n",
       "std       2.842763     5.869047     4.310030     2.101013     0.497103\n",
       "min      -7.042100   -13.773100    -5.286100    -8.548200     0.000000\n",
       "25%      -1.773000    -1.708200    -1.574975    -2.413450     0.000000\n",
       "50%       0.496180     2.319650     0.616630    -0.586650     0.000000\n",
       "75%       2.821475     6.814625     3.179250     0.394810     1.000000\n",
       "max       6.824800    12.951600    17.927400     2.449500     1.000000"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "banknote_datadset.describe()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Then We extract the train data from the dataframe"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-09-21T06:29:19.324495Z",
     "start_time": "2020-09-21T06:29:19.319829Z"
    }
   },
   "outputs": [],
   "source": [
    "X = banknote_datadset.iloc[:, 0:4].values \n",
    "y = banknote_datadset.iloc[:, 4].values "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-09-21T06:29:19.769041Z",
     "start_time": "2020-09-21T06:29:19.763510Z"
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "1372"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(X)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We have 1372 instances. We are going to use the build-in GridSearch of LionForests to find and train the best classifier for this dataset"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Machine Learning models training step"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We will use a MinMax scaler to normalize the input"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-09-21T06:29:21.680980Z",
     "start_time": "2020-09-21T06:29:21.677031Z"
    }
   },
   "outputs": [],
   "source": [
    "scaler = MinMaxScaler(feature_range=(-1,1))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We are going to use 4 different classifiers (Random Forests, SVMs, Logistic Regression, Neural Networks"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-09-21T06:29:22.128410Z",
     "start_time": "2020-09-21T06:29:22.124606Z"
    }
   },
   "outputs": [],
   "source": [
    "classifiers = {}\n",
    "scalers = {}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-09-21T06:29:27.938595Z",
     "start_time": "2020-09-21T06:29:22.903974Z"
    }
   },
   "outputs": [],
   "source": [
    "pipe = Pipeline(steps=[('scaler', scaler), ('rf', RandomForestClassifier(random_state=77))])\n",
    "parameters =[{\n",
    "    'rf__max_depth': [10],#1, 5, 7, 10\n",
    "    'rf__max_features': [0.75], #'sqrt', 'log2', 0.75, None\n",
    "    'rf__bootstrap': [True], #True, False\n",
    "    'rf__min_samples_leaf' : [1], #1, 2, 5, 10, 0.10\n",
    "    'rf__n_estimators': [500] #10, 100, 500, 1000\n",
    "}]\n",
    "clf = GridSearchCV(pipe, parameters, scoring='f1', cv=10, n_jobs=-1)\n",
    "clf.fit(X, y)\n",
    "scaler_rf = clf.best_estimator_.steps[0][1]\n",
    "rf = clf.best_estimator_.steps[1][1]\n",
    "classifiers[1] = [rf, str(\"Random Forests: \"+ str(clf.best_score_))]\n",
    "scalers[1] = scaler_rf"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-09-21T06:29:28.254565Z",
     "start_time": "2020-09-21T06:29:28.146778Z"
    }
   },
   "outputs": [],
   "source": [
    "pipe = Pipeline(steps=[('scaler', scaler), ('svm', SVC(probability=True,random_state=77))])\n",
    "#parameters = [\n",
    "#  {'svm__C': [-3, 1, 3, 10, 100, 1000], 'svm__kernel': ['linear']},\n",
    "#  {'svm__C': [-3, 1, 3, 10, 100, 1000], 'svm__gamma': [0.1, 0.01, 0.001, 0.0001], 'svm__kernel': ['rbf']},\n",
    "#]\n",
    "parameters = {'svm__C': [100], 'svm__gamma': [0.1], 'svm__kernel': ['rbf']} #best\n",
    "clf = GridSearchCV(pipe, parameters, scoring='f1', cv=10, n_jobs=-1)\n",
    "clf.fit(X, y)\n",
    "scaler_svm = clf.best_estimator_.steps[0][1]\n",
    "svm = clf.best_estimator_.steps[1][1]\n",
    "classifiers[2] = [svm, str(\"SVM: \"+ str(clf.best_score_))]\n",
    "scalers[2] = scaler_svm"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-09-21T06:29:28.490905Z",
     "start_time": "2020-09-21T06:29:28.436498Z"
    }
   },
   "outputs": [],
   "source": [
    "pipe = Pipeline(steps=[('scaler', scaler), ('lr', LogisticRegression(random_state=77))])\n",
    "#parameters = [\n",
    "#  {'lr__C': [-3, 1, 3, 10, 100, 1000], 'lr__penalty': ['l1'], 'lr__solver': ['liblinear', 'saga']},\n",
    "#  {'lr__C': [-3, 1, 3, 10, 100, 1000], 'lr__penalty': ['l2'], 'lr__solver': ['newton-cg', 'lbfgs', 'sag','saga']}\n",
    "#]\n",
    "parameters = {'lr__C': [3], 'lr__penalty': ['l1'], 'lr__solver': ['liblinear']}#best\n",
    "clf = GridSearchCV(pipe, parameters, scoring='f1', cv=10, n_jobs=-1)\n",
    "clf.fit(X, y)\n",
    "scaler_lr = clf.best_estimator_.steps[0][1]\n",
    "lr = clf.best_estimator_.steps[1][1]\n",
    "classifiers[3] = [lr, str(\"Logistic Regression: \"+ str(clf.best_score_))]\n",
    "scalers[3] = scaler_lr"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-09-21T06:29:37.572011Z",
     "start_time": "2020-09-21T06:29:32.629452Z"
    }
   },
   "outputs": [],
   "source": [
    "pipe = Pipeline(steps=[('scaler', scaler), ('nn', MLPClassifier(early_stopping=True, random_state=77))])\n",
    "#parameters = {\n",
    "#    'nn__hidden_layer_sizes': [(2,10),(5,10),(10,100),(20,200),(50,500)], \n",
    "#    'nn__activation': ['logistic', 'tanh', 'relu'],\n",
    "#    'nn__solver': ['sgd', 'adam'],\n",
    "#    'nn__alpha': [0.000001,0.0001,0.001, 0.01, 0.1],\n",
    "#    'nn__learning_rate': ['constant', 'invscaling', 'adaptive']}\n",
    "parameters = {\n",
    "    'nn__hidden_layer_sizes': [(100,1000)], \n",
    "    'nn__activation': ['relu'],\n",
    "    'nn__solver': ['adam'],\n",
    "    'nn__alpha': [0.0001],\n",
    "    'nn__learning_rate': ['constant']}\n",
    "clf = GridSearchCV(pipe, parameters, scoring='f1', cv=10, n_jobs=-1)\n",
    "clf.fit(X, y)\n",
    "scaler_nn = clf.best_estimator_.steps[0][1]\n",
    "nn = clf.best_estimator_.steps[1][1]\n",
    "classifiers[4] = [nn, str(\"Neural Network: \"+ str(clf.best_score_))]\n",
    "scalers[4] = scaler_nn"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We can see the best classifiers of each algorithm and their scores. In this dataset SVMs work better than the others!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-09-21T06:29:43.460264Z",
     "start_time": "2020-09-21T06:29:43.451140Z"
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{1: [RandomForestClassifier(max_depth=10, max_features=0.75, n_estimators=500,\n",
       "                         random_state=77),\n",
       "  'Random Forests: 0.9926289484206319'],\n",
       " 2: [SVC(C=100, gamma=0.1, probability=True, random_state=77), 'SVM: 1.0'],\n",
       " 3: [LogisticRegression(C=3, penalty='l1', random_state=77, solver='liblinear'),\n",
       "  'Logistic Regression: 0.9886410119993929'],\n",
       " 4: [MLPClassifier(early_stopping=True, hidden_layer_sizes=(100, 1000),\n",
       "                random_state=77), 'Neural Network: 0.9943351691581432']}"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "classifiers"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-09-20T21:00:12.307139Z",
     "start_time": "2020-09-20T21:00:12.301480Z"
    }
   },
   "source": [
    "## Quantitative Tests"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Finally, we run the quantitative experimennts for these classifiers and 3 to 4 interpretation techniques. All the models and the techniques will be tested on a commno subset of the original data, in order to check which technique provided less untruthful features for each model. This is a way to select model and interpretation technique, as well as a way to benchmark different interpretation techniques. For a qualitative and more informative example of Altruist and its explanations please open the Qualitative.ipynb"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-09-20T21:00:13.569939Z",
     "start_time": "2020-09-20T21:00:12.458036Z"
    }
   },
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "e77bf0521b124d3bb8be7eaab6d08780",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "interactive(children=(Checkbox(value=False, description='eli_5'), Checkbox(value=True, description='lime'), Ch…"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "@interact(eli_5=False, lime=True, shap=True, perm_importance=True, intristic=True, cl=(1,4))\n",
    "def g(eli_5, lime, shap, perm_importance, intristic, cl=1):\n",
    "    print(classifiers[cl][1])\n",
    "    X_t = scalers[cl].transform(X)\n",
    "    fi = FeatureImportance(X_t, y, feature_names, class_names)\n",
    "    fi_names = {fi.fi_lime:'Lime',fi.fi_shap:'Shap',fi.fi_eli:'Eli5',fi.fi_perm_imp:'Permuation Importance',fi.fi_rf:'Pseudo-Intristic RFs', fi.fi_coef_lr:'Intristic LR'}\n",
    "    fis = []\n",
    "    if (eli_5 and not cl == 2 and not cl == 3):\n",
    "        fis.append(fi.fi_eli)\n",
    "    if lime:\n",
    "        fis.append(fi.fi_lime)\n",
    "    if shap:\n",
    "        fis.append(fi.fi_shap)\n",
    "    if perm_importance:\n",
    "        fis.append(fi.fi_perm_imp)\n",
    "    if intristic and cl == 1:\n",
    "        fis.append(fi.fi_rf)\n",
    "    if intristic and cl == 3:\n",
    "        fis.append(fi.fi_coef_lr)\n",
    "    fis_scores = []\n",
    "    for i in fis:\n",
    "        fis_scores.append([])\n",
    "    count = 0\n",
    "    for instance in X_t:\n",
    "        if (count + 1) % 100 == 0:\n",
    "            print(count+1,\"/\",len(X_t),\"..\",end=\", \")\n",
    "        count = count + 1\n",
    "        altruistino = Altruist(classifiers[cl][0], X_t, fis, feature_names, None)\n",
    "        untruthful_features = altruistino.find_untruthful_features(instance)\n",
    "        for i in range(len(untruthful_features[0])):\n",
    "            fis_scores[i].append(len(untruthful_features[0][i]))\n",
    "    print(len(X_t),\"/\",len(X_t))\n",
    "    count = 0\n",
    "    for fis_score in fis_scores:\n",
    "        fi = fis[count]\n",
    "        count = count + 1\n",
    "        print(fi_names[fi],np.array(fis_score).mean())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Heart Statlog Dataset"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "About Heart Statlog: This dataset is for binary classification tasks to predict the absence or presence of a heart disease. This dataset is a heart disease database similar to a database already present in the repository (Heart Disease databases) but in a slightly different form. \n",
    "\n",
    "Attribute Information:\n",
    "1. age\n",
    "2. sex\n",
    "3. chest pain type\n",
    "4. resting blood pressure\n",
    "5. serum cholesterol in mg/dl\n",
    "6. fasting blood sugar > 120 mg/dl\n",
    "7. resting electrocardiographic results (values 0,1,2)\n",
    "8. maximum heart rate achieved\n",
    "9. exercise induced angina\n",
    "10. oldpeak = ST depression induced by exercise relative to rest\n",
    "11. the slope of the peak exercise ST segment\n",
    "12. number of major vessels (0-3) colored by flourosopy\n",
    "13. thal: 3 = normal; 6 = fixed defect; 7 = reversable defect \n",
    "\n",
    "Source: https://archive.ics.uci.edu/ml/datasets/statlog+(heart)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Data Loading"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Firstly, we load the dataset and we set the feature and class names"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-09-21T06:29:54.897807Z",
     "start_time": "2020-09-21T06:29:54.298008Z"
    }
   },
   "outputs": [],
   "source": [
    "url=\"http://archive.ics.uci.edu/ml/machine-learning-databases/statlog/heart/heart.dat\"\n",
    "raw_data = urllib.request.urlopen(url)\n",
    "credit=np.genfromtxt(raw_data)\n",
    "X,y = credit[:,:-1], credit[:,-1].squeeze()\n",
    "y = [int(i-1) for i in y]\n",
    "feature_names = ['age','sex','chest pain','resting blood pressure','serum cholestoral',\n",
    "               'fasting blood sugar','resting ecg results','maximum heart rate achieved','exercise induced angina','oldpeak',\n",
    "               'the slope of the peak exercise','number of major vessels','reversable defect']\n",
    "class_names = ['absence','presence']\n",
    "\n",
    "heart_statlog = pd.DataFrame(X,columns=feature_names)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We can plot some instances to see the features and their values"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-09-21T06:29:55.718630Z",
     "start_time": "2020-09-21T06:29:55.699062Z"
    }
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>age</th>\n",
       "      <th>sex</th>\n",
       "      <th>chest pain</th>\n",
       "      <th>resting blood pressure</th>\n",
       "      <th>serum cholestoral</th>\n",
       "      <th>fasting blood sugar</th>\n",
       "      <th>resting ecg results</th>\n",
       "      <th>maximum heart rate achieved</th>\n",
       "      <th>exercise induced angina</th>\n",
       "      <th>oldpeak</th>\n",
       "      <th>the slope of the peak exercise</th>\n",
       "      <th>number of major vessels</th>\n",
       "      <th>reversable defect</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>70.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>4.0</td>\n",
       "      <td>130.0</td>\n",
       "      <td>322.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>2.0</td>\n",
       "      <td>109.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>2.4</td>\n",
       "      <td>2.0</td>\n",
       "      <td>3.0</td>\n",
       "      <td>3.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>67.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>3.0</td>\n",
       "      <td>115.0</td>\n",
       "      <td>564.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>2.0</td>\n",
       "      <td>160.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.6</td>\n",
       "      <td>2.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>7.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>57.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>2.0</td>\n",
       "      <td>124.0</td>\n",
       "      <td>261.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>141.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.3</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>7.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>64.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>4.0</td>\n",
       "      <td>128.0</td>\n",
       "      <td>263.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>105.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.2</td>\n",
       "      <td>2.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>7.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>74.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>2.0</td>\n",
       "      <td>120.0</td>\n",
       "      <td>269.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>2.0</td>\n",
       "      <td>121.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.2</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>3.0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "    age  sex  chest pain  resting blood pressure  serum cholestoral  \\\n",
       "0  70.0  1.0         4.0                   130.0              322.0   \n",
       "1  67.0  0.0         3.0                   115.0              564.0   \n",
       "2  57.0  1.0         2.0                   124.0              261.0   \n",
       "3  64.0  1.0         4.0                   128.0              263.0   \n",
       "4  74.0  0.0         2.0                   120.0              269.0   \n",
       "\n",
       "   fasting blood sugar  resting ecg results  maximum heart rate achieved  \\\n",
       "0                  0.0                  2.0                        109.0   \n",
       "1                  0.0                  2.0                        160.0   \n",
       "2                  0.0                  0.0                        141.0   \n",
       "3                  0.0                  0.0                        105.0   \n",
       "4                  0.0                  2.0                        121.0   \n",
       "\n",
       "   exercise induced angina  oldpeak  the slope of the peak exercise  \\\n",
       "0                      0.0      2.4                             2.0   \n",
       "1                      0.0      1.6                             2.0   \n",
       "2                      0.0      0.3                             1.0   \n",
       "3                      1.0      0.2                             2.0   \n",
       "4                      1.0      0.2                             1.0   \n",
       "\n",
       "   number of major vessels  reversable defect  \n",
       "0                      3.0                3.0  \n",
       "1                      0.0                7.0  \n",
       "2                      0.0                7.0  \n",
       "3                      1.0                7.0  \n",
       "4                      1.0                3.0  "
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "heart_statlog.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Moreover, we can use pandas.describe() to see the ranges of each feature. For example, we observe that curtosis's range is -5.286 to 17.927"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-09-21T06:29:57.169517Z",
     "start_time": "2020-09-21T06:29:57.125444Z"
    }
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>age</th>\n",
       "      <th>sex</th>\n",
       "      <th>chest pain</th>\n",
       "      <th>resting blood pressure</th>\n",
       "      <th>serum cholestoral</th>\n",
       "      <th>fasting blood sugar</th>\n",
       "      <th>resting ecg results</th>\n",
       "      <th>maximum heart rate achieved</th>\n",
       "      <th>exercise induced angina</th>\n",
       "      <th>oldpeak</th>\n",
       "      <th>the slope of the peak exercise</th>\n",
       "      <th>number of major vessels</th>\n",
       "      <th>reversable defect</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>count</th>\n",
       "      <td>270.000000</td>\n",
       "      <td>270.000000</td>\n",
       "      <td>270.000000</td>\n",
       "      <td>270.000000</td>\n",
       "      <td>270.000000</td>\n",
       "      <td>270.000000</td>\n",
       "      <td>270.000000</td>\n",
       "      <td>270.000000</td>\n",
       "      <td>270.000000</td>\n",
       "      <td>270.00000</td>\n",
       "      <td>270.000000</td>\n",
       "      <td>270.000000</td>\n",
       "      <td>270.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>mean</th>\n",
       "      <td>54.433333</td>\n",
       "      <td>0.677778</td>\n",
       "      <td>3.174074</td>\n",
       "      <td>131.344444</td>\n",
       "      <td>249.659259</td>\n",
       "      <td>0.148148</td>\n",
       "      <td>1.022222</td>\n",
       "      <td>149.677778</td>\n",
       "      <td>0.329630</td>\n",
       "      <td>1.05000</td>\n",
       "      <td>1.585185</td>\n",
       "      <td>0.670370</td>\n",
       "      <td>4.696296</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>std</th>\n",
       "      <td>9.109067</td>\n",
       "      <td>0.468195</td>\n",
       "      <td>0.950090</td>\n",
       "      <td>17.861608</td>\n",
       "      <td>51.686237</td>\n",
       "      <td>0.355906</td>\n",
       "      <td>0.997891</td>\n",
       "      <td>23.165717</td>\n",
       "      <td>0.470952</td>\n",
       "      <td>1.14521</td>\n",
       "      <td>0.614390</td>\n",
       "      <td>0.943896</td>\n",
       "      <td>1.940659</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>min</th>\n",
       "      <td>29.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>94.000000</td>\n",
       "      <td>126.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>71.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.00000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>3.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>25%</th>\n",
       "      <td>48.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>3.000000</td>\n",
       "      <td>120.000000</td>\n",
       "      <td>213.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>133.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.00000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>3.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>50%</th>\n",
       "      <td>55.000000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>3.000000</td>\n",
       "      <td>130.000000</td>\n",
       "      <td>245.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>2.000000</td>\n",
       "      <td>153.500000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.80000</td>\n",
       "      <td>2.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>3.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>75%</th>\n",
       "      <td>61.000000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>4.000000</td>\n",
       "      <td>140.000000</td>\n",
       "      <td>280.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>2.000000</td>\n",
       "      <td>166.000000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>1.60000</td>\n",
       "      <td>2.000000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>7.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>max</th>\n",
       "      <td>77.000000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>4.000000</td>\n",
       "      <td>200.000000</td>\n",
       "      <td>564.000000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>2.000000</td>\n",
       "      <td>202.000000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>6.20000</td>\n",
       "      <td>3.000000</td>\n",
       "      <td>3.000000</td>\n",
       "      <td>7.000000</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "              age         sex  chest pain  resting blood pressure  \\\n",
       "count  270.000000  270.000000  270.000000              270.000000   \n",
       "mean    54.433333    0.677778    3.174074              131.344444   \n",
       "std      9.109067    0.468195    0.950090               17.861608   \n",
       "min     29.000000    0.000000    1.000000               94.000000   \n",
       "25%     48.000000    0.000000    3.000000              120.000000   \n",
       "50%     55.000000    1.000000    3.000000              130.000000   \n",
       "75%     61.000000    1.000000    4.000000              140.000000   \n",
       "max     77.000000    1.000000    4.000000              200.000000   \n",
       "\n",
       "       serum cholestoral  fasting blood sugar  resting ecg results  \\\n",
       "count         270.000000           270.000000           270.000000   \n",
       "mean          249.659259             0.148148             1.022222   \n",
       "std            51.686237             0.355906             0.997891   \n",
       "min           126.000000             0.000000             0.000000   \n",
       "25%           213.000000             0.000000             0.000000   \n",
       "50%           245.000000             0.000000             2.000000   \n",
       "75%           280.000000             0.000000             2.000000   \n",
       "max           564.000000             1.000000             2.000000   \n",
       "\n",
       "       maximum heart rate achieved  exercise induced angina    oldpeak  \\\n",
       "count                   270.000000               270.000000  270.00000   \n",
       "mean                    149.677778                 0.329630    1.05000   \n",
       "std                      23.165717                 0.470952    1.14521   \n",
       "min                      71.000000                 0.000000    0.00000   \n",
       "25%                     133.000000                 0.000000    0.00000   \n",
       "50%                     153.500000                 0.000000    0.80000   \n",
       "75%                     166.000000                 1.000000    1.60000   \n",
       "max                     202.000000                 1.000000    6.20000   \n",
       "\n",
       "       the slope of the peak exercise  number of major vessels  \\\n",
       "count                      270.000000               270.000000   \n",
       "mean                         1.585185                 0.670370   \n",
       "std                          0.614390                 0.943896   \n",
       "min                          1.000000                 0.000000   \n",
       "25%                          1.000000                 0.000000   \n",
       "50%                          2.000000                 0.000000   \n",
       "75%                          2.000000                 1.000000   \n",
       "max                          3.000000                 3.000000   \n",
       "\n",
       "       reversable defect  \n",
       "count         270.000000  \n",
       "mean            4.696296  \n",
       "std             1.940659  \n",
       "min             3.000000  \n",
       "25%             3.000000  \n",
       "50%             3.000000  \n",
       "75%             7.000000  \n",
       "max             7.000000  "
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "heart_statlog.describe()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We have 270 instances. We are going to use the build-in GridSearch of LionForests to find and train the best classifier for this dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-09-21T06:29:58.142494Z",
     "start_time": "2020-09-21T06:29:58.136192Z"
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "270"
      ]
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(X)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Machine Learning models training step"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We will use a MinMax scaler to normalize the input"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-09-21T06:29:59.817777Z",
     "start_time": "2020-09-21T06:29:59.813610Z"
    }
   },
   "outputs": [],
   "source": [
    "scaler = MinMaxScaler(feature_range=(-1,1))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We are going to use 4 different classifiers (Random Forests, SVMs, Logistic Regression, Neural Networks"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-09-21T06:30:00.650327Z",
     "start_time": "2020-09-21T06:30:00.647170Z"
    }
   },
   "outputs": [],
   "source": [
    "classifiers = {}\n",
    "scalers = {}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-09-21T06:30:03.265662Z",
     "start_time": "2020-09-21T06:30:01.236755Z"
    }
   },
   "outputs": [],
   "source": [
    "pipe = Pipeline(steps=[('scaler', scaler), ('rf', RandomForestClassifier(random_state=0))])\n",
    "parameters =[{\n",
    "    'rf__max_depth': [5],#1, 5, 7, 10\n",
    "    'rf__max_features': ['sqrt'], #'sqrt', 'log2', 0.75, None\n",
    "    'rf__bootstrap': [False], #True, False\n",
    "    'rf__min_samples_leaf' : [5], #1, 2, 5, 10, 0.10\n",
    "    'rf__n_estimators': [500] #10, 100, 500, 1000\n",
    "}]\n",
    "clf = GridSearchCV(pipe, parameters, scoring='f1', cv=10, n_jobs=-1)\n",
    "clf.fit(X, y)\n",
    "scaler_rf = clf.best_estimator_.steps[0][1]\n",
    "rf = clf.best_estimator_.steps[1][1]\n",
    "classifiers[1] = [rf, str(\"Random Forests: \"+ str(clf.best_score_))]\n",
    "scalers[1] = scaler_rf"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-09-21T06:30:03.643259Z",
     "start_time": "2020-09-21T06:30:03.588923Z"
    }
   },
   "outputs": [],
   "source": [
    "pipe = Pipeline(steps=[('scaler', scaler), ('svm', SVC(probability=True,random_state=77))])\n",
    "#parameters = [\n",
    "#  {'svm__C': [-3, 1, 3, 10, 100, 1000], 'svm__kernel': ['linear']},\n",
    "#  {'svm__C': [-3, 1, 3, 10, 100, 1000], 'svm__gamma': [0.1, 0.01, 0.001, 0.0001], 'svm__kernel': ['rbf']},\n",
    "#]\n",
    "parameters = {'svm__C': [100], 'svm__gamma': [0.001], 'svm__kernel': ['rbf']} #best\n",
    "clf = GridSearchCV(pipe, parameters, scoring='f1', cv=10, n_jobs=-1)\n",
    "clf.fit(X, y)\n",
    "scaler_svm = clf.best_estimator_.steps[0][1]\n",
    "svm = clf.best_estimator_.steps[1][1]\n",
    "classifiers[2] = [svm, str(\"SVM: \"+ str(clf.best_score_))]\n",
    "scalers[2] = scaler_svm"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-09-21T06:30:03.982830Z",
     "start_time": "2020-09-21T06:30:03.939033Z"
    }
   },
   "outputs": [],
   "source": [
    "pipe = Pipeline(steps=[('scaler', scaler), ('lr', LogisticRegression(random_state=77))])\n",
    "#parameters = [\n",
    "#  {'lr__C': [-3, 1, 3, 10, 100, 1000], 'lr__penalty': ['l1'], 'lr__solver': ['liblinear', 'saga']},\n",
    "#  {'lr__C': [-3, 1, 3, 10, 100, 1000], 'lr__penalty': ['l2'], 'lr__solver': ['newton-cg', 'lbfgs', 'sag','saga']}\n",
    "#]\n",
    "parameters = {'lr__C': [1], 'lr__penalty': ['l1'], 'lr__solver': ['saga']}#best\n",
    "clf = GridSearchCV(pipe, parameters, scoring='f1', cv=10, n_jobs=-1)\n",
    "clf.fit(X, y)\n",
    "scaler_lr = clf.best_estimator_.steps[0][1]\n",
    "lr = clf.best_estimator_.steps[1][1]\n",
    "classifiers[3] = [lr, str(\"Logistic Regression: \"+ str(clf.best_score_))]\n",
    "scalers[3] = scaler_lr"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-09-21T06:30:06.347947Z",
     "start_time": "2020-09-21T06:30:05.108290Z"
    }
   },
   "outputs": [],
   "source": [
    "pipe = Pipeline(steps=[('scaler', scaler), ('nn', MLPClassifier(early_stopping=True, random_state=77))])\n",
    "#parameters = {\n",
    "#    'nn__hidden_layer_sizes': [(2,10),(5,10),(10,100),(20,200),(50,500), (100,1000)],\n",
    "#    'nn__activation': ['logistic', 'tanh', 'relu'],\n",
    "#    'nn__solver': ['sgd', 'adam'],\n",
    "#    'nn__alpha': [0.000001,0.0001,0.001, 0.01, 0.1],\n",
    "#    'nn__learning_rate': ['constant', 'invscaling', 'adaptive']}\n",
    "parameters = {\n",
    "    'nn__hidden_layer_sizes': [(100,1000)], \n",
    "    'nn__activation': ['tanh'],\n",
    "    'nn__solver': ['adam'],\n",
    "    'nn__alpha': [0.000001],\n",
    "    'nn__learning_rate': ['constant']}\n",
    "clf = GridSearchCV(pipe, parameters, scoring='f1', cv=10, n_jobs=-1)\n",
    "clf.fit(X, y)\n",
    "scaler_nn = clf.best_estimator_.steps[0][1]\n",
    "nn = clf.best_estimator_.steps[1][1]\n",
    "classifiers[4] = [nn, str(\"Neural Network: \"+ str(clf.best_score_))]\n",
    "scalers[4] = scaler_nn"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We can see the best classifiers of each algorithm and their scores. In this dataset SVMs work better than the others!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-09-21T06:30:08.524260Z",
     "start_time": "2020-09-21T06:30:08.515107Z"
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{1: [RandomForestClassifier(bootstrap=False, max_depth=5, max_features='sqrt',\n",
       "                         min_samples_leaf=5, n_estimators=500, random_state=0),\n",
       "  'Random Forests: 0.8188916011524707'],\n",
       " 2: [SVC(C=100, gamma=0.001, probability=True, random_state=77),\n",
       "  'SVM: 0.8195089355089354'],\n",
       " 3: [LogisticRegression(C=1, penalty='l1', random_state=77, solver='saga'),\n",
       "  'Logistic Regression: 0.8120600762065292'],\n",
       " 4: [MLPClassifier(activation='tanh', alpha=1e-06, early_stopping=True,\n",
       "                hidden_layer_sizes=(100, 1000), random_state=77),\n",
       "  'Neural Network: 0.7701775669029673']}"
      ]
     },
     "execution_count": 25,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "classifiers"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Quantitative Tests"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Finally, we run the quantitative experimennts for these classifiers and 3 to 4 interpretation techniques. All the models and the techniques will be tested on a commno subset of the original data, in order to check which technique provided less untruthful features for each model. This is a way to select model and interpretation technique, as well as a way to benchmark different interpretation techniques. For a qualitative and more informative example of Altruist and its explanations please open the Qualitative.ipynb"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-09-21T06:12:09.856045Z",
     "start_time": "2020-09-21T06:12:02.172302Z"
    },
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "1377f1d2d8474113ba380fb6de520c31",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "interactive(children=(Checkbox(value=False, description='eli_5'), Checkbox(value=True, description='lime'), Ch…"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "@interact(eli_5=False, lime=True, shap=True, perm_importance=True, intristic=True, cl=(1,4))\n",
    "def g(eli_5, lime, shap, perm_importance, intristic, cl=1):\n",
    "    print(classifiers[cl][1])\n",
    "    X_t = scalers[cl].transform(X)\n",
    "    fi = FeatureImportance(X_t, y, feature_names, class_names)\n",
    "    fi_names = {fi.fi_lime:'Lime',fi.fi_shap:'Shap',fi.fi_eli:'Eli5',fi.fi_perm_imp:'Permuation Importance',fi.fi_rf:'Pseudo-Intristic RFs', fi.fi_coef_lr:'Intristic LR'}\n",
    "    fis = []\n",
    "    if (eli_5 and not cl == 2 and not cl == 3):\n",
    "        fis.append(fi.fi_eli)\n",
    "    if lime:\n",
    "        fis.append(fi.fi_lime)\n",
    "    if shap:\n",
    "        fis.append(fi.fi_shap)\n",
    "    if perm_importance:\n",
    "        fis.append(fi.fi_perm_imp)\n",
    "    if intristic and cl == 1:\n",
    "        fis.append(fi.fi_rf)\n",
    "    if intristic and cl == 3:\n",
    "        fis.append(fi.fi_coef_lr)\n",
    "    fis_scores = []\n",
    "    for i in fis:\n",
    "        fis_scores.append([])\n",
    "    count = 0\n",
    "    for instance in X_t:\n",
    "        if (count + 1) % 50 == 0:\n",
    "            print(count+1,\"/\",len(X_t),\"..\",end=\", \")\n",
    "        count = count + 1\n",
    "        altruistino = Altruist(classifiers[cl][0], X_t, fis, feature_names, None)\n",
    "        untruthful_features = altruistino.find_untruthful_features(instance)\n",
    "        for i in range(len(untruthful_features[0])):\n",
    "            fis_scores[i].append(len(untruthful_features[0][i]))\n",
    "    print(len(X_t),\"/\",len(X_t))\n",
    "    count = 0\n",
    "    for fis_score in fis_scores:\n",
    "        fi = fis[count]\n",
    "        count = count + 1\n",
    "        print(fi_names[fi],np.array(fis_score).mean())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Adult Census Dataset"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "About Adult Census: This dataset is for a binary classification task. Predict whether income exceeds $50K/yr based on census data. Also known as \"Census Income\" dataset.\n",
    "\n",
    "Attribute Information:\n",
    "\n",
    "1. age: continuous.\n",
    "2. workclass: Private, Self-emp-not-inc, Self-emp-inc, Federal-gov, Local-gov, State-gov, Without-pay, Never-worked.\n",
    "3. fnlwgt: continuous.\n",
    "4. education: Bachelors, Some-college, 11th, HS-grad, Prof-school, Assoc-acdm, Assoc-voc, 9th, 7th-8th, 12th, Masters, 1st-4th, 10th, Doctorate, 5th-6th, Preschool.\n",
    "5. education-num: continuous.\n",
    "6. marital-status: Married-civ-spouse, Divorced, Never-married, Separated, Widowed, Married-spouse-absent, Married-AF-spouse.\n",
    "occupation: Tech-support, Craft-repair, Other-service, Sales, Exec-managerial, Prof-specialty, Handlers-cleaners, Machine-op-inspct, Adm-clerical, Farming-fishing, Transport-moving, Priv-house-serv, Protective-serv, Armed-Forces.\n",
    "7. relationship: Wife, Own-child, Husband, Not-in-family, Other-relative, Unmarried.\n",
    "8. race: White, Asian-Pac-Islander, Amer-Indian-Eskimo, Other, Black.\n",
    "9. sex: Female, Male.\n",
    "10. capital-gain: continuous.\n",
    "11. capital-loss: continuous.\n",
    "12. hours-per-week: continuous.\n",
    "13. native-country: United-States, Cambodia, England, Puerto-Rico, Canada, Germany, Outlying-US(Guam-USVI-etc), India, Japan, Greece, South, China, Cuba, Iran, Honduras, Philippines, Italy, Poland, Jamaica, Vietnam, Mexico, Portugal, Ireland, France, Dominican-Republic, Laos, Ecuador, Taiwan, Haiti, Columbia, Hungary, Guatemala, Nicaragua, Scotland, Thailand, Yugoslavia, El-Salvador, Trinadad&Tobago, Peru, Hong, Holand-Netherlands.\n",
    "\n",
    "Source: https://archive.ics.uci.edu/ml/datasets/adult"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Data Loading"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Firstly, we load the dataset and we set the feature and class names"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-09-21T06:31:04.982822Z",
     "start_time": "2020-09-21T06:30:54.743664Z"
    }
   },
   "outputs": [],
   "source": [
    "feature_names = ['age', 'workclass', 'fnlwgt', 'education', 'education-num', 'marital-status', 'occupation', 'relationship', 'race', 'sex', 'capital-gain', 'capital-loss', 'hours-per-week', 'native-country','salary']\n",
    "class_names=['<=50K','>50K'] #0: <=50K and 1: >50K\n",
    "data = pd.read_csv('https://archive.ics.uci.edu/ml/machine-learning-databases/adult/adult.data', names=feature_names, delimiter=', ')\n",
    "data_test = pd.read_csv('https://archive.ics.uci.edu/ml/machine-learning-databases/adult/adult.test', names=feature_names, delimiter=', ')\n",
    "data_test = data_test.drop(data_test.index[[0]])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We are doing the following preprocessing influenced by a github notebook"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-09-21T06:31:22.673019Z",
     "start_time": "2020-09-21T06:31:22.504812Z"
    }
   },
   "outputs": [],
   "source": [
    "data = data[(data != '?').all(axis=1)]\n",
    "data_test = data_test[(data_test != '?').all(axis=1)]\n",
    "data_test['salary'] = data_test['salary'].map({'<=50K.': '<=50K', '>50K.': '>50K'})\n",
    "frames = [data, data_test]\n",
    "data = pd.concat(frames)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Feature Engineering from:\n",
    "https://github.com/pooja2512/Adult-Census-Income/blob/master/Adult%20Census%20Income.ipynb. So run and skip the next code block"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-09-21T06:31:25.130449Z",
     "start_time": "2020-09-21T06:31:24.613193Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of observations in final dataset: (45167, 82)\n"
     ]
    }
   ],
   "source": [
    "hs_grad = ['HS-grad','11th','10th','9th','12th']\n",
    "elementary = ['1st-4th','5th-6th','7th-8th']\n",
    "# replace elements in list.\n",
    "for i in hs_grad:\n",
    "    data['education'].replace(i , 'HS-grad', regex=True , inplace=True)\n",
    "for e in elementary:\n",
    "    data['education'].replace(e , 'elementary-school', regex=True, inplace = True)\n",
    "\n",
    "married= ['Married-spouse-absent','Married-civ-spouse','Married-AF-spouse']\n",
    "separated = ['Separated','Divorced']\n",
    "#replace elements in list.\n",
    "for m in married:\n",
    "    data['marital-status'].replace(m ,'Married', regex=True, inplace = True)\n",
    "for s in separated:\n",
    "    data['marital-status'].replace(s ,'Separated', regex=True, inplace = True)\n",
    "\n",
    "self_employed = ['Self-emp-not-inc','Self-emp-inc']\n",
    "govt_employees = ['Local-gov','State-gov','Federal-gov']\n",
    "for se in self_employed:\n",
    "    data['workclass'].replace(se , 'Self_employed', regex=True, inplace = True)\n",
    "for ge in govt_employees:\n",
    "    data['workclass'].replace(ge , 'Govt_employees', regex=True, inplace = True)\n",
    "\n",
    "del_cols = ['relationship','education-num']\n",
    "data.drop(labels = del_cols, axis = 1, inplace = True)\n",
    "\n",
    "index_age = data[data['age'] == 90].index\n",
    "data.drop(labels = index_age, axis = 0, inplace =True)\n",
    "num_col_new = ['age','capital-gain', 'capital-loss',\n",
    "       'hours-per-week','fnlwgt']\n",
    "cat_col_new = ['workclass', 'education', 'marital-status', 'occupation',\n",
    "               'race', 'sex','salary','native-country']#add native-country label\n",
    "scaler = MinMaxScaler()\n",
    "#pd.DataFrame(scaler.fit_transform(data[num_col_new]),columns = num_col_new)\n",
    "class DataFrameSelector(TransformerMixin):\n",
    "    def __init__(self,attribute_names):\n",
    "        self.attribute_names = attribute_names\n",
    "    def fit(self,X,y = None):\n",
    "        return self\n",
    "    def transform(self,X):\n",
    "        return X[self.attribute_names]\n",
    "class num_trans(TransformerMixin):\n",
    "    def __init__(self):\n",
    "        pass\n",
    "    def fit(self,X,y=None):\n",
    "        return self\n",
    "    def transform(self,X):\n",
    "        df = pd.DataFrame(X)\n",
    "        df.columns = num_col_new \n",
    "        return df\n",
    "pipeline = Pipeline([('selector',DataFrameSelector(num_col_new)),  \n",
    "                     ('scaler',MinMaxScaler()),('transform',num_trans())])#('scaler',MinMaxScaler()),        \n",
    "num_df = pipeline.fit_transform(data)\n",
    "num_df.shape\n",
    "# columns which I don't need after creating dummy variables dataframe\n",
    "cols = ['workclass_Govt_employess','education_Some-college',\n",
    "        'marital-status_Never-married','occupation_Other-service',\n",
    "        'race_Black','sex_Male','salary_>50K']\n",
    "class dummies(TransformerMixin):\n",
    "    def __init__(self,cols):\n",
    "        self.cols = cols\n",
    "    \n",
    "    def fit(self,X,y = None):\n",
    "        return self\n",
    "    \n",
    "    def transform(self,X):\n",
    "        df = pd.get_dummies(X)\n",
    "        df_new = df[df.columns.difference(cols)] \n",
    "        return df_new\n",
    "pipeline_cat=Pipeline([('selector',DataFrameSelector(cat_col_new)),\n",
    "                      ('dummies',dummies(cols))])\n",
    "cat_df = pipeline_cat.fit_transform(data)\n",
    "cat_df['id'] = pd.Series(range(cat_df.shape[0]))\n",
    "num_df['id'] = pd.Series(range(num_df.shape[0]))\n",
    "final_df = pd.merge(cat_df,num_df,how = 'inner', on = 'id')\n",
    "print(f\"Number of observations in final dataset: {final_df.shape}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We extract the train and target data from the dataframe"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-09-21T06:31:26.163717Z",
     "start_time": "2020-09-21T06:31:26.140618Z"
    }
   },
   "outputs": [],
   "source": [
    "y = final_df['salary_<=50K'].values\n",
    "final_df.drop(labels = ['id','salary_<=50K'],axis = 1,inplace = True)\n",
    "X = final_df.values"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-09-21T06:31:27.151263Z",
     "start_time": "2020-09-21T06:31:27.147213Z"
    }
   },
   "outputs": [],
   "source": [
    "feature_names = list(final_df.columns.values)\n",
    "categorical_features = ['workclass', 'education', 'marital-status', 'occupation', 'race', 'sex','native-country']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-09-21T06:31:28.126862Z",
     "start_time": "2020-09-21T06:31:28.121851Z"
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "45167"
      ]
     },
     "execution_count": 31,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(X)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-09-21T06:35:43.536635Z",
     "start_time": "2020-09-21T06:31:29.142131Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Original dataset shape Counter({1: 33970, 0: 11197})\n",
      "TomekLinks: Resampled dataset shape Counter({1: 31155, 0: 11197})\n",
      "NC: Resampled dataset shape Counter({1: 25878, 0: 11197})\n",
      "NM: Resampled dataset shape Counter({0: 11197, 1: 7017})\n",
      "Random: Resampled dataset shape Counter({0: 7017, 1: 7017})\n"
     ]
    }
   ],
   "source": [
    "from imblearn.under_sampling import TomekLinks, NeighbourhoodCleaningRule, NearMiss,RandomUnderSampler\n",
    "from collections import Counter\n",
    "\n",
    "print('Original dataset shape %s' % Counter(y))\n",
    "\n",
    "tl = TomekLinks()\n",
    "X_res, y_res = tl.fit_resample(X, y)\n",
    "print('TomekLinks: Resampled dataset shape %s' % Counter(y_res))\n",
    "ncr = NeighbourhoodCleaningRule()\n",
    "X_res, y_res = ncr.fit_resample(X_res, y_res)\n",
    "print('NC: Resampled dataset shape %s' % Counter(y_res))\n",
    "nm = NearMiss(version=3)\n",
    "X_res, y_res = nm.fit_resample(X_res, y_res)\n",
    "print('NM: Resampled dataset shape %s' % Counter(y_res))\n",
    "rus = RandomUnderSampler(random_state=42)\n",
    "X_res, y_res = rus.fit_resample(X_res, y_res)\n",
    "print('Random: Resampled dataset shape %s' % Counter(y_res))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-09-21T06:35:44.566671Z",
     "start_time": "2020-09-21T06:35:44.555586Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Random: Resampled dataset shape Counter({0: 1983, 1: 1983})\n"
     ]
    }
   ],
   "source": [
    "rus = RandomUnderSampler(sampling_strategy='all', random_state=42)\n",
    "X_res2, y_res2 = rus.fit_resample(X_res[:9000], y_res[:9000])\n",
    "print('Random: Resampled dataset shape %s' % Counter(y_res2))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-09-21T06:35:45.539008Z",
     "start_time": "2020-09-21T06:35:45.529292Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Random: Resampled dataset shape Counter({0: 500, 1: 500})\n"
     ]
    }
   ],
   "source": [
    "rus = RandomUnderSampler(sampling_strategy='all', random_state=42)\n",
    "X_res2, y_res2 = rus.fit_resample(X_res[:7517], y_res[:7517])\n",
    "print('Random: Resampled dataset shape %s' % Counter(y_res2))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We have 1372 instances. We are going to use the build-in GridSearch of LionForests to find and train the best classifier for this dataset"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Machine Learning models training step"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We will use a MinMax scaler to normalize the input\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-09-21T06:35:46.550635Z",
     "start_time": "2020-09-21T06:35:46.546193Z"
    }
   },
   "outputs": [],
   "source": [
    "scaler = MinMaxScaler(feature_range=(-1,1))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We are going to use 4 different classifiers (Random Forests, SVMs, Logistic Regression, Neural Networks"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-09-21T06:35:47.516110Z",
     "start_time": "2020-09-21T06:35:47.511777Z"
    }
   },
   "outputs": [],
   "source": [
    "classifiers = {}\n",
    "scalers = {}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-09-21T06:35:49.674585Z",
     "start_time": "2020-09-21T06:35:48.453125Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Fitting 10 folds for each of 1 candidates, totalling 10 fits\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[Parallel(n_jobs=-1)]: Using backend LokyBackend with 8 concurrent workers.\n",
      "[Parallel(n_jobs=-1)]: Done   6 out of  10 | elapsed:    1.1s remaining:    0.8s\n",
      "[Parallel(n_jobs=-1)]: Done  10 out of  10 | elapsed:    1.2s finished\n"
     ]
    }
   ],
   "source": [
    "pipe = Pipeline(steps=[('scaler', scaler), ('rf', RandomForestClassifier(random_state=77))])\n",
    "parameters =[{\n",
    "    'rf__max_depth': [7], #1, 5, 7, 10\n",
    "    'rf__max_features': ['sqrt'], #'sqrt', 'log2', 0.75, None\n",
    "    'rf__bootstrap': [False], #True, False\n",
    "    'rf__min_samples_leaf' : [2], #1, 2, 5, 10, 0.10\n",
    "    'rf__n_estimators': [10] #10, 100, 500, 1000\n",
    "}]\n",
    "clf = GridSearchCV(pipe, parameters, scoring='f1', cv=10, n_jobs=-1, verbose=1)\n",
    "clf.fit(X_res2, y_res2)\n",
    "scaler_rf = clf.best_estimator_.steps[0][1]\n",
    "rf = clf.best_estimator_.steps[1][1]\n",
    "classifiers[1] = [rf, str(\"Random Forests: \"+ str(clf.best_score_))]\n",
    "scalers[1] = scaler_rf"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-09-21T06:35:51.347691Z",
     "start_time": "2020-09-21T06:35:50.629328Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Fitting 10 folds for each of 1 candidates, totalling 10 fits\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[Parallel(n_jobs=-1)]: Using backend LokyBackend with 8 concurrent workers.\n",
      "[Parallel(n_jobs=-1)]: Done   6 out of  10 | elapsed:    0.3s remaining:    0.2s\n",
      "[Parallel(n_jobs=-1)]: Done  10 out of  10 | elapsed:    0.5s finished\n"
     ]
    }
   ],
   "source": [
    "pipe = Pipeline(steps=[('scaler', scaler), ('svm', SVC(probability=True,random_state=77))])\n",
    "parameters = [\n",
    "  {'svm__C': [-3, 1, 3, 10, 100, 1000], 'svm__kernel': ['linear']},\n",
    "  {'svm__C': [-3, 1, 3, 10, 100, 1000], 'svm__gamma': [0.1, 0.01, 0.001, 0.0001], 'svm__kernel': ['rbf']},\n",
    "]\n",
    "parameters = {'svm__C': [3], 'svm__gamma': [0.1], 'svm__kernel': ['rbf']} #best\n",
    "clf = GridSearchCV(pipe, parameters, scoring='f1', cv=10, n_jobs=-1, verbose=1)\n",
    "clf.fit(X_res2, y_res2)\n",
    "scaler_svm = clf.best_estimator_.steps[0][1]\n",
    "svm = clf.best_estimator_.steps[1][1]\n",
    "classifiers[2] = [svm, str(\"SVM: \"+ str(clf.best_score_))]\n",
    "scalers[2] = scaler_svm"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-09-21T06:35:52.658530Z",
     "start_time": "2020-09-21T06:35:52.285351Z"
    },
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Fitting 10 folds for each of 1 candidates, totalling 10 fits\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[Parallel(n_jobs=-1)]: Using backend LokyBackend with 8 concurrent workers.\n",
      "[Parallel(n_jobs=-1)]: Done   6 out of  10 | elapsed:    0.2s remaining:    0.1s\n",
      "[Parallel(n_jobs=-1)]: Done  10 out of  10 | elapsed:    0.3s finished\n"
     ]
    }
   ],
   "source": [
    "pipe = Pipeline(steps=[('scaler', scaler), ('lr', LogisticRegression(random_state=77))])\n",
    "parameters = [\n",
    "  {'lr__C': [-3, 1, 3, 10, 100, 1000], 'lr__penalty': ['l1'], 'lr__solver': ['liblinear', 'saga']},\n",
    "  {'lr__C': [-3, 1, 3, 10, 100, 1000], 'lr__penalty': ['l2'], 'lr__solver': ['newton-cg', 'lbfgs', 'sag','saga']}\n",
    "]\n",
    "parameters = {'lr__C': [3], 'lr__penalty': ['l2'], 'lr__solver': ['sag']}#best\n",
    "clf = GridSearchCV(pipe, parameters, scoring='f1', cv=10, n_jobs=-1, verbose=1)\n",
    "clf.fit(X_res2, y_res2)\n",
    "scaler_lr = clf.best_estimator_.steps[0][1]\n",
    "lr = clf.best_estimator_.steps[1][1]\n",
    "classifiers[3] = [lr, str(\"Logistic Regression: \"+ str(clf.best_score_))]\n",
    "scalers[3] = scaler_lr"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-09-21T06:35:56.462248Z",
     "start_time": "2020-09-21T06:35:53.890534Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Fitting 10 folds for each of 1 candidates, totalling 10 fits\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[Parallel(n_jobs=-1)]: Using backend LokyBackend with 8 concurrent workers.\n",
      "[Parallel(n_jobs=-1)]: Done   6 out of  10 | elapsed:    1.5s remaining:    1.0s\n",
      "[Parallel(n_jobs=-1)]: Done  10 out of  10 | elapsed:    2.3s finished\n"
     ]
    }
   ],
   "source": [
    "pipe = Pipeline(steps=[('scaler', scaler), ('nn', MLPClassifier(early_stopping=True, random_state=77))])\n",
    "parameters = {\n",
    "    'nn__hidden_layer_sizes': [(2,10),(5,10),(10,100),(20,200),(50,500),(100,1000)], \n",
    "    'nn__activation': ['logistic', 'tanh', 'relu'],\n",
    "    'nn__solver': ['sgd', 'adam'],\n",
    "    'nn__alpha': [0.000001,0.0001,0.001, 0.01, 0.1],\n",
    "    'nn__learning_rate': ['constant', 'invscaling', 'adaptive']}\n",
    "parameters = {\n",
    "    'nn__hidden_layer_sizes': [(50,500)], \n",
    "    'nn__activation': ['tanh'],\n",
    "    'nn__solver': ['adam'],\n",
    "    'nn__alpha': [0.000001],\n",
    "    'nn__learning_rate': ['constant']}\n",
    "clf = GridSearchCV(pipe, parameters, scoring='f1', cv=10, n_jobs=-1, verbose=1)\n",
    "clf.fit(X_res2, y_res2)\n",
    "\n",
    "scaler_nn = clf.best_estimator_.steps[0][1]\n",
    "nn = clf.best_estimator_.steps[1][1]\n",
    "classifiers[4] = [nn, str(\"Neural Network: \"+ str(clf.best_score_))]\n",
    "scalers[4] = scaler_nn"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-09-21T06:35:57.567323Z",
     "start_time": "2020-09-21T06:35:57.559467Z"
    }
   },
   "source": [
    "We can see the best classifiers of each algorithm and their scores. In this dataset SVMs work better than the others!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-09-21T06:36:04.563939Z",
     "start_time": "2020-09-21T06:36:04.554804Z"
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{1: [RandomForestClassifier(bootstrap=False, max_depth=7, max_features='sqrt',\n",
       "                         min_samples_leaf=2, n_estimators=10, random_state=77),\n",
       "  'Random Forests: 0.9431755994216005'],\n",
       " 2: [SVC(C=3, gamma=0.1, probability=True, random_state=77),\n",
       "  'SVM: 0.9593078770419098'],\n",
       " 3: [LogisticRegression(C=3, random_state=77, solver='sag'),\n",
       "  'Logistic Regression: 0.9467018455060752'],\n",
       " 4: [MLPClassifier(activation='tanh', alpha=1e-06, early_stopping=True,\n",
       "                hidden_layer_sizes=(50, 500), random_state=77),\n",
       "  'Neural Network: 0.9442092900222173']}"
      ]
     },
     "execution_count": 42,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "classifiers"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Quantitative Tests"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Finally, we run the quantitative experimennts for these classifiers and 3 to 4 interpretation techniques. All the models and the techniques will be tested on a commno subset of the original data, in order to check which technique provided less untruthful features for each model. This is a way to select model and interpretation technique, as well as a way to benchmark different interpretation techniques. For a qualitative and more informative example of Altruist and its explanations please open the Qualitative.ipynb"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "34fded6ea2834e249f6476067b25d50d",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "interactive(children=(Checkbox(value=False, description='eli_5'), Checkbox(value=True, description='lime'), Ch…"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "@interact(eli_5=False, lime=True, shap=True, perm_importance=True, intristic=True, cl=(1,4))\n",
    "def g(eli_5, lime, shap, perm_importance, intristic, cl=2):\n",
    "    print(classifiers[cl][1])\n",
    "    X_t = scalers[cl].transform(X_res2)\n",
    "    fi = FeatureImportance(X_t[900:1000], y_res2[900:1000], feature_names, class_names)\n",
    "    fi_names = {fi.fi_lime:'Lime',fi.fi_shap:'Shap',fi.fi_eli:'Eli5',fi.fi_perm_imp:'Permuation Importance',fi.fi_rf:'Pseudo-Intristic RFs', fi.fi_coef_lr:'Intristic LR'}\n",
    "    fis = []\n",
    "    if (eli_5 and not cl == 2 and not cl == 3):\n",
    "        fis.append(fi.fi_eli)\n",
    "    if lime:\n",
    "        fis.append(fi.fi_lime)\n",
    "    if shap:\n",
    "        fis.append(fi.fi_shap)\n",
    "    if perm_importance:\n",
    "        fis.append(fi.fi_perm_imp)\n",
    "    if intristic and cl == 1:\n",
    "        fis.append(fi.fi_rf)\n",
    "    if intristic and cl == 3:\n",
    "        fis.append(fi.fi_coef_lr)\n",
    "    fis_scores = []\n",
    "    for i in fis:\n",
    "        fis_scores.append([])\n",
    "    count = 0;\n",
    "    altruistino = Altruist(classifiers[cl][0], X_t, fis, feature_names,None)\n",
    "    for instance in X_t[900:1000]:\n",
    "        if (count + 1) % 10 == 0:\n",
    "            print(count+1,\"/\",len(X_t),\"..\",end=\", \")\n",
    "        count = count + 1\n",
    "        untruthful_features = altruistino.find_untruthful_features(instance)\n",
    "        for i in range(len(untruthful_features[0])):\n",
    "            fis_scores[i].append(len(untruthful_features[0][i]))\n",
    "    print(len(X_t),\"/\",len(X_t))\n",
    "    count = 0\n",
    "    for fis_score in fis_scores:\n",
    "        fi = fis[count]\n",
    "        count = count + 1\n",
    "        print(fi_names[fi],np.array(fis_score).mean())\n",
    "        print(fi_names[fi],np.array(fis_score).sum())\n",
    "        print(count)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.8"
  },
  "toc": {
   "base_numbering": 1,
   "nav_menu": {},
   "number_sections": true,
   "sideBar": true,
   "skip_h1_title": false,
   "title_cell": "Table of Contents",
   "title_sidebar": "Contents",
   "toc_cell": false,
   "toc_position": {},
   "toc_section_display": true,
   "toc_window_display": false
  },
  "varInspector": {
   "cols": {
    "lenName": 16,
    "lenType": 16,
    "lenVar": 40
   },
   "kernels_config": {
    "python": {
     "delete_cmd_postfix": "",
     "delete_cmd_prefix": "del ",
     "library": "var_list.py",
     "varRefreshCmd": "print(var_dic_list())"
    },
    "r": {
     "delete_cmd_postfix": ") ",
     "delete_cmd_prefix": "rm(",
     "library": "var_list.r",
     "varRefreshCmd": "cat(var_dic_list()) "
    }
   },
   "types_to_exclude": [
    "module",
    "function",
    "builtin_function_or_method",
    "instance",
    "_Feature"
   ],
   "window_display": false
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
