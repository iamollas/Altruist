{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# This notebook contains the experiments on Banknote dataset with Altruist"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Load few libraries we will need"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-09-20T20:59:54.021596Z",
     "start_time": "2020-09-20T20:59:54.018455Z"
    }
   },
   "outputs": [],
   "source": [
    "import warnings\n",
    "warnings.filterwarnings(\"ignore\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-09-20T20:59:57.164363Z",
     "start_time": "2020-09-20T20:59:54.432855Z"
    }
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Using TensorFlow backend.\n"
     ]
    }
   ],
   "source": [
    "from sklearn.preprocessing import StandardScaler, MinMaxScaler\n",
    "from sklearn.pipeline import Pipeline\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.ensemble import GradientBoostingClassifier\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "from sklearn.neural_network import MLPClassifier\n",
    "from sklearn.model_selection import GridSearchCV\n",
    "from sklearn.svm import SVC\n",
    "from sklearn.model_selection import train_test_split\n",
    "from altruist import Altruist\n",
    "from fi_techniques import FeatureImportance\n",
    "import pandas as pd \n",
    "import numpy as np\n",
    "import urllib\n",
    "import networkx as nx\n",
    "import matplotlib.pyplot as plt\n",
    "from ipywidgets import interact, interactive, fixed, interact_manual\n",
    "import ipywidgets as widgets"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Data Loading"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Firstly, we load the dataset and we set the feature and class names"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-09-20T20:59:57.628086Z",
     "start_time": "2020-09-20T20:59:57.199839Z"
    }
   },
   "outputs": [],
   "source": [
    "banknote_datadset = pd.read_csv('https://raw.githubusercontent.com/Kuntal-G/Machine-Learning/master/R-machine-learning/data/banknote-authentication.csv')\n",
    "feature_names = ['variance','skew','curtosis','entropy']\n",
    "class_names=['fake banknote','real banknote'] #0: no, 1: yes #or ['not authenticated banknote','authenticated banknote']"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We can plot some instances to see the features and their values"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-09-20T20:59:57.802253Z",
     "start_time": "2020-09-20T20:59:57.790615Z"
    }
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>variance</th>\n",
       "      <th>skew</th>\n",
       "      <th>curtosis</th>\n",
       "      <th>entropy</th>\n",
       "      <th>class</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>3.62160</td>\n",
       "      <td>8.6661</td>\n",
       "      <td>-2.8073</td>\n",
       "      <td>-0.44699</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>4.54590</td>\n",
       "      <td>8.1674</td>\n",
       "      <td>-2.4586</td>\n",
       "      <td>-1.46210</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>3.86600</td>\n",
       "      <td>-2.6383</td>\n",
       "      <td>1.9242</td>\n",
       "      <td>0.10645</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>3.45660</td>\n",
       "      <td>9.5228</td>\n",
       "      <td>-4.0112</td>\n",
       "      <td>-3.59440</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>0.32924</td>\n",
       "      <td>-4.4552</td>\n",
       "      <td>4.5718</td>\n",
       "      <td>-0.98880</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   variance    skew  curtosis  entropy  class\n",
       "0   3.62160  8.6661   -2.8073 -0.44699      0\n",
       "1   4.54590  8.1674   -2.4586 -1.46210      0\n",
       "2   3.86600 -2.6383    1.9242  0.10645      0\n",
       "3   3.45660  9.5228   -4.0112 -3.59440      0\n",
       "4   0.32924 -4.4552    4.5718 -0.98880      0"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "banknote_datadset.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Moreover, we can use pandas.describe() to see the ranges of each feature. For example, we observe that curtosis's range is -5.286 to 17.927"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-09-20T20:59:58.671971Z",
     "start_time": "2020-09-20T20:59:58.642786Z"
    }
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>variance</th>\n",
       "      <th>skew</th>\n",
       "      <th>curtosis</th>\n",
       "      <th>entropy</th>\n",
       "      <th>class</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>count</th>\n",
       "      <td>1372.000000</td>\n",
       "      <td>1372.000000</td>\n",
       "      <td>1372.000000</td>\n",
       "      <td>1372.000000</td>\n",
       "      <td>1372.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>mean</th>\n",
       "      <td>0.433735</td>\n",
       "      <td>1.922353</td>\n",
       "      <td>1.397627</td>\n",
       "      <td>-1.191657</td>\n",
       "      <td>0.444606</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>std</th>\n",
       "      <td>2.842763</td>\n",
       "      <td>5.869047</td>\n",
       "      <td>4.310030</td>\n",
       "      <td>2.101013</td>\n",
       "      <td>0.497103</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>min</th>\n",
       "      <td>-7.042100</td>\n",
       "      <td>-13.773100</td>\n",
       "      <td>-5.286100</td>\n",
       "      <td>-8.548200</td>\n",
       "      <td>0.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>25%</th>\n",
       "      <td>-1.773000</td>\n",
       "      <td>-1.708200</td>\n",
       "      <td>-1.574975</td>\n",
       "      <td>-2.413450</td>\n",
       "      <td>0.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>50%</th>\n",
       "      <td>0.496180</td>\n",
       "      <td>2.319650</td>\n",
       "      <td>0.616630</td>\n",
       "      <td>-0.586650</td>\n",
       "      <td>0.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>75%</th>\n",
       "      <td>2.821475</td>\n",
       "      <td>6.814625</td>\n",
       "      <td>3.179250</td>\n",
       "      <td>0.394810</td>\n",
       "      <td>1.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>max</th>\n",
       "      <td>6.824800</td>\n",
       "      <td>12.951600</td>\n",
       "      <td>17.927400</td>\n",
       "      <td>2.449500</td>\n",
       "      <td>1.000000</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "          variance         skew     curtosis      entropy        class\n",
       "count  1372.000000  1372.000000  1372.000000  1372.000000  1372.000000\n",
       "mean      0.433735     1.922353     1.397627    -1.191657     0.444606\n",
       "std       2.842763     5.869047     4.310030     2.101013     0.497103\n",
       "min      -7.042100   -13.773100    -5.286100    -8.548200     0.000000\n",
       "25%      -1.773000    -1.708200    -1.574975    -2.413450     0.000000\n",
       "50%       0.496180     2.319650     0.616630    -0.586650     0.000000\n",
       "75%       2.821475     6.814625     3.179250     0.394810     1.000000\n",
       "max       6.824800    12.951600    17.927400     2.449500     1.000000"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "banknote_datadset.describe()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Then We extract the train data from the dataframe"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-09-20T20:59:59.292227Z",
     "start_time": "2020-09-20T20:59:59.286588Z"
    }
   },
   "outputs": [],
   "source": [
    "X = banknote_datadset.iloc[:, 0:4].values \n",
    "y = banknote_datadset.iloc[:, 4].values "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-09-20T20:59:59.534192Z",
     "start_time": "2020-09-20T20:59:59.528111Z"
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "1372"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(X)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We have 1372 instances. We are going to use the build-in GridSearch of LionForests to find and train the best classifier for this dataset"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Machine Learning models training step"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We will use a MinMax scaler to normalize the input"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-09-20T21:00:00.288262Z",
     "start_time": "2020-09-20T21:00:00.282868Z"
    }
   },
   "outputs": [],
   "source": [
    "scaler = MinMaxScaler(feature_range=(-1,1))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We are going to use 4 different classifiers (Random Forests, SVMs, Logistic Regression, Neural Networks"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-09-20T21:00:00.518459Z",
     "start_time": "2020-09-20T21:00:00.514958Z"
    }
   },
   "outputs": [],
   "source": [
    "classifiers = {}\n",
    "scalers = {}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-09-20T21:00:06.411822Z",
     "start_time": "2020-09-20T21:00:00.785234Z"
    }
   },
   "outputs": [],
   "source": [
    "pipe = Pipeline(steps=[('scaler', scaler), ('rf', RandomForestClassifier(random_state=77))])\n",
    "parameters =[{\n",
    "    'rf__max_depth': [10],#1, 5, 7, 10\n",
    "    'rf__max_features': [0.75], #'sqrt', 'log2', 0.75, None\n",
    "    'rf__bootstrap': [True], #True, False\n",
    "    'rf__min_samples_leaf' : [1], #1, 2, 5, 10, 0.10\n",
    "    'rf__n_estimators': [500] #10, 100, 500, 1000\n",
    "}]\n",
    "clf = GridSearchCV(pipe, parameters, scoring='f1', cv=10, n_jobs=-1)\n",
    "clf.fit(X, y)\n",
    "scaler_rf = clf.best_estimator_.steps[0][1]\n",
    "rf = clf.best_estimator_.steps[1][1]\n",
    "classifiers[1] = [rf, str(\"Random Forests: \"+ str(clf.best_score_))]\n",
    "scalers[1] = scaler_rf"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-09-20T21:00:06.770664Z",
     "start_time": "2020-09-20T21:00:06.667638Z"
    }
   },
   "outputs": [],
   "source": [
    "pipe = Pipeline(steps=[('scaler', scaler), ('svm', SVC(probability=True,random_state=77))])\n",
    "#parameters = [\n",
    "#  {'svm__C': [-3, 1, 3, 10, 100, 1000], 'svm__kernel': ['linear']},\n",
    "#  {'svm__C': [-3, 1, 3, 10, 100, 1000], 'svm__gamma': [0.1, 0.01, 0.001, 0.0001], 'svm__kernel': ['rbf']},\n",
    "#]\n",
    "parameters = {'svm__C': [100], 'svm__gamma': [0.1], 'svm__kernel': ['rbf']} #best\n",
    "clf = GridSearchCV(pipe, parameters, scoring='f1', cv=10, n_jobs=-1)\n",
    "clf.fit(X, y)\n",
    "scaler_svm = clf.best_estimator_.steps[0][1]\n",
    "svm = clf.best_estimator_.steps[1][1]\n",
    "classifiers[2] = [svm, str(\"SVM: \"+ str(clf.best_score_))]\n",
    "scalers[2] = scaler_svm"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-09-20T21:00:06.959788Z",
     "start_time": "2020-09-20T21:00:06.902925Z"
    }
   },
   "outputs": [],
   "source": [
    "pipe = Pipeline(steps=[('scaler', scaler), ('lr', LogisticRegression(random_state=77))])\n",
    "#parameters = [\n",
    "#  {'lr__C': [-3, 1, 3, 10, 100, 1000], 'lr__penalty': ['l1'], 'lr__solver': ['liblinear', 'saga']},\n",
    "#  {'lr__C': [-3, 1, 3, 10, 100, 1000], 'lr__penalty': ['l2'], 'lr__solver': ['newton-cg', 'lbfgs', 'sag','saga']}\n",
    "#]\n",
    "parameters = {'lr__C': [3], 'lr__penalty': ['l1'], 'lr__solver': ['liblinear']}#best\n",
    "clf = GridSearchCV(pipe, parameters, scoring='f1', cv=10, n_jobs=-1)\n",
    "clf.fit(X, y)\n",
    "scaler_lr = clf.best_estimator_.steps[0][1]\n",
    "lr = clf.best_estimator_.steps[1][1]\n",
    "classifiers[3] = [lr, str(\"Logistic Regression: \"+ str(clf.best_score_))]\n",
    "scalers[3] = scaler_lr"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-09-20T21:00:12.003299Z",
     "start_time": "2020-09-20T21:00:07.103052Z"
    }
   },
   "outputs": [],
   "source": [
    "pipe = Pipeline(steps=[('scaler', scaler), ('nn', MLPClassifier(early_stopping=True, random_state=77))])\n",
    "#parameters = {\n",
    "#    'nn__hidden_layer_sizes': [(2,10),(5,10),(10,100),(20,200),(50,500)], \n",
    "#    'nn__activation': ['logistic', 'tanh', 'relu'],\n",
    "#    'nn__solver': ['sgd', 'adam'],\n",
    "#    'nn__alpha': [0.000001,0.0001,0.001, 0.01, 0.1],\n",
    "#    'nn__learning_rate': ['constant', 'invscaling', 'adaptive']}\n",
    "parameters = {\n",
    "    'nn__hidden_layer_sizes': [(100,1000)], \n",
    "    'nn__activation': ['relu'],\n",
    "    'nn__solver': ['adam'],\n",
    "    'nn__alpha': [0.0001],\n",
    "    'nn__learning_rate': ['constant']}\n",
    "clf = GridSearchCV(pipe, parameters, scoring='f1', cv=10, n_jobs=-1)\n",
    "clf.fit(X, y)\n",
    "scaler_nn = clf.best_estimator_.steps[0][1]\n",
    "nn = clf.best_estimator_.steps[1][1]\n",
    "classifiers[4] = [nn, str(\"Neural Network: \"+ str(clf.best_score_))]\n",
    "scalers[4] = scaler_nn"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-09-20T21:00:12.157891Z",
     "start_time": "2020-09-20T21:00:12.151269Z"
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{1: [RandomForestClassifier(max_depth=10, max_features=0.75, n_estimators=500,\n",
       "                         random_state=77),\n",
       "  'Random Forests: 0.9926289484206319'],\n",
       " 2: [SVC(C=100, gamma=0.1, probability=True, random_state=77), 'SVM: 1.0'],\n",
       " 3: [LogisticRegression(C=3, penalty='l1', random_state=77, solver='liblinear'),\n",
       "  'Logistic Regression: 0.9886410119993929'],\n",
       " 4: [MLPClassifier(early_stopping=True, hidden_layer_sizes=(100, 1000),\n",
       "                random_state=77), 'Neural Network: 0.9943351691581432']}"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "classifiers"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-09-20T21:00:12.307139Z",
     "start_time": "2020-09-20T21:00:12.301480Z"
    }
   },
   "source": [
    "## Quantitative Tests"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Finally, we run the quantitative experimennts for these classifiers and 3 to 4 interpretation techniques. All the models and the techniques will be tested on a commno subset of the original data, in order to check which technique provided less untruthful features for each model. This is a way to select model and interpretation technique, as well as a way to benchmark different interpretation techniques. For a qualitative and more informative example of Altruist and its explanations please open the Qualitative.ipynb"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-09-20T21:00:13.569939Z",
     "start_time": "2020-09-20T21:00:12.458036Z"
    }
   },
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "e77bf0521b124d3bb8be7eaab6d08780",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "interactive(children=(Checkbox(value=False, description='eli_5'), Checkbox(value=True, description='lime'), Châ€¦"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "@interact(eli_5=False, lime=True, shap=True, perm_importance=True, intristic=True, cl=(1,4))\n",
    "def g(eli_5, lime, shap, perm_importance, intristic, cl=1):\n",
    "    print(classifiers[cl][1])\n",
    "    X_t = scalers[cl].transform(X)\n",
    "    fi = FeatureImportance(X_t, y, feature_names, class_names)\n",
    "    fi_names = {fi.fi_lime:'Lime',fi.fi_shap:'Shap',fi.fi_eli:'Eli5',fi.fi_perm_imp:'Permuation Importance',fi.fi_rf:'Pseudo-Intristic RFs', fi.fi_coef_lr:'Intristic LR'}\n",
    "    fis = []\n",
    "    if (eli_5 and not cl == 2 and not cl == 3):\n",
    "        fis.append(fi.fi_eli)\n",
    "    if lime:\n",
    "        fis.append(fi.fi_lime)\n",
    "    if shap:\n",
    "        fis.append(fi.fi_shap)\n",
    "    if perm_importance:\n",
    "        fis.append(fi.fi_perm_imp)\n",
    "    if intristic and cl == 1:\n",
    "        fis.append(fi.fi_rf)\n",
    "    if intristic and cl == 3:\n",
    "        fis.append(fi.fi_coef_lr)\n",
    "    fis_scores = []\n",
    "    for i in fis:\n",
    "        fis_scores.append([])\n",
    "    count = 0\n",
    "    for instance in X_t:\n",
    "        if (count + 1) % 100 == 0:\n",
    "            print(count+1,\"/\",len(X_t),\"..\",end=\", \")\n",
    "        count = count + 1\n",
    "        altruistino = Altruist(classifiers[cl][0], X_t, fis, feature_names, None)\n",
    "        untruthful_features = altruistino.find_untruthful_features(instance)\n",
    "        for i in range(len(untruthful_features[0])):\n",
    "            fis_scores[i].append(len(untruthful_features[0][i]))\n",
    "    print(len(X_t),\"/\",len(X_t))\n",
    "    count = 0\n",
    "    for fis_score in fis_scores:\n",
    "        fi = fis[count]\n",
    "        count = count + 1\n",
    "        print(fi_names[fi],np.array(fis_score).mean())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.8"
  },
  "toc": {
   "base_numbering": 1,
   "nav_menu": {},
   "number_sections": true,
   "sideBar": true,
   "skip_h1_title": false,
   "title_cell": "Table of Contents",
   "title_sidebar": "Contents",
   "toc_cell": false,
   "toc_position": {},
   "toc_section_display": true,
   "toc_window_display": false
  },
  "varInspector": {
   "cols": {
    "lenName": 16,
    "lenType": 16,
    "lenVar": 40
   },
   "kernels_config": {
    "python": {
     "delete_cmd_postfix": "",
     "delete_cmd_prefix": "del ",
     "library": "var_list.py",
     "varRefreshCmd": "print(var_dic_list())"
    },
    "r": {
     "delete_cmd_postfix": ") ",
     "delete_cmd_prefix": "rm(",
     "library": "var_list.r",
     "varRefreshCmd": "cat(var_dic_list()) "
    }
   },
   "types_to_exclude": [
    "module",
    "function",
    "builtin_function_or_method",
    "instance",
    "_Feature"
   ],
   "window_display": false
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
